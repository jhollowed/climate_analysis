{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1ae9a997-0864-4160-9110-4fb3901fd179",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdb\n",
    "import glob\n",
    "import copy\n",
    "import timeit\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import cartopy.crs as ccrs\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "from matplotlib.colors import TwoSlopeNorm\n",
    "%matplotlib inline\n",
    "\n",
    "#from dask.distributed import Client\n",
    "#client = Client()  # start a local Dask client\n",
    "#print(client)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82111c15-2623-4fba-8561-46ace387750d",
   "metadata": {},
   "source": [
    "## Specify data directories. \n",
    "\n",
    "According to Andrew Schuh:\n",
    "\n",
    "> File locations on hercules:\n",
    "> \n",
    "> `/work/noaa/co2/aschuh/WOMBAT_stuff/wombat-v3-forward`\n",
    "> \n",
    "> `/work/noaa/co2/aschuh/WOMBAT_stuff/wombat-v3-inverse`\n",
    "> \n",
    "> 4D Jacobian fields will be in `forward` and the postprocessed observational Jacobians will be in `inverse`.\n",
    "\n",
    "> The TM5 versions are in\n",
    "> \n",
    "> `/work/noaa/co2/andy/Projects/WOMBAT/wombat-v3-forward`\n",
    "> \n",
    "> Transport stuff is in 3b. Output sensitivities are in 4b.\n",
    "\n",
    "And according to Mike Bertolacci:\n",
    "\n",
    "> There are three run directories, all in\n",
    ">\n",
    "> `3a_transport_gc/intermediates`:\n",
    "> \n",
    "> - `runs`: most of the basis-function runs are here. These are split out by gpp/resp/ocean, region, pft for resp and gpp, component (intercept/trend/sin/cos/residual), and for the residual by month. Some combinations of region and pft are omitted due to negligible fluxes.\n",
    ">\n",
    "> - `runs-correction`: adds a few cases missing from the above\n",
    ">\n",
    "> - `runs-control`: has \"control runs\" where each major component + inventory is run, plus fires and fossil fluxes. These are not split out by region or month, and cover some cases that are omitted in the jacobian due to negligible fluxes.\n",
    "\n",
    "### Update 12/8/25:\n",
    "\n",
    "Mike suggests looking only at the control runs for now, and investigating both the mole fraction and flux data in tandem to understand what's happening. The flux field (emissions corresponding to each basis function) are in the `HEMCO_diagnostics` files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "007f0050-a79c-46db-9de3-ca874252287e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '/work/noaa/co2/aschuh/WOMBAT_stuff'\n",
    "inverse_dir = f'{data_dir}/wombat-v3-inverse'\n",
    "forward_dir = f'{data_dir}/wombat-v3-forward'\n",
    "gc_transport_dir = f'{forward_dir}/3a_transport_gc/intermediates/runs-control'\n",
    "\n",
    "def lev_to_p(lev):\n",
    "    '''\n",
    "    Converts level index to pressure in hPa\n",
    "    '''\n",
    "    df = pd.read_fwf(\"levels47.csv\",skiprows=3,\n",
    "                     names=[\"L\", \"eta_edge\", \"eta_mid\", \"alt_km\", \"p_hpa\"])\n",
    "    # keep only midpoint rows\n",
    "    mid = df[df[\"eta_mid\"].notna()].reset_index(drop=True)\n",
    "    # get pressure\n",
    "    return float(mid.loc[mid[\"L\"] == lev, \"p_hpa\"].iloc[0])\n",
    "def lev_to_z(lev):\n",
    "    '''\n",
    "    Converts level index to altitude in km\n",
    "    '''\n",
    "    df = pd.read_fwf(\"levels47.csv\",skiprows=3,\n",
    "                     names=[\"L\", \"eta_edge\", \"eta_mid\", \"alt_km\", \"p_hpa\"])\n",
    "    # keep only midpoint rows\n",
    "    mid = df[df[\"eta_mid\"].notna()].reset_index(drop=True)\n",
    "    # get pressure\n",
    "    return float(mid.loc[mid[\"L\"] == lev, \"alt_km\"].iloc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d696f497-df90-43c1-9024-4f65787e4fa4",
   "metadata": {},
   "source": [
    "## Choose source, read components via tracer mappings from file\n",
    "\n",
    "According to Mike:\n",
    "\n",
    "> For each run directory there is a mapping.csv file that tells you how things work.\n",
    "\n",
    "> The first few lines are:\n",
    ">\n",
    "> `---`\n",
    "> \n",
    "> `basis_function, run, species`\n",
    "> \n",
    "> `background_climatology_20140901, climatology_20140901_part001, r0001p001s001`\n",
    "> \n",
    ">`ocean_intercept_regionRegion01, climatology_20140901_part001, r0001p001s002`\n",
    ">\n",
    "> `---`\n",
    ">\n",
    "> Here:\n",
    "> \n",
    "> - The first column tells you the name of the basis function\n",
    "> \n",
    "> - The second column tells you which run directory to find it in\n",
    ">\n",
    "> - The third column tells you the tracer name within that run\n",
    ">\n",
    "> - Here, the first `background` row is a simple run with no fluxes starting with a 400ppm IC\n",
    ">\n",
    "> - The second row is one for the ocean intercept in region 1 (you can tell from the name).\n",
    ">\n",
    "> All basis function runs start from a 400ppm IC\n",
    "\n",
    "\n",
    "> Let's try to find the `ocean_intercept_regionRegion01` tracer:\n",
    "> \n",
    "> - In `3a_transport_gc/intermediates/runs/` you will find directories like: `climatology_20140901_part001_split01`, `climatology_20140901_part001_split02`, and so on\n",
    ">\n",
    "> - This is a split over time, here each run runs for 3 months\n",
    "Inside each directory you can find an `OutputDir`, which has daily files like `GEOSChem.SpeciesConcThreeHourly.YYYYMMDD_0000z.nc4`; these are the 3 hourly concentrations\n",
    ">\n",
    "> - In these files you will find `SpeciesConcVV_r0001p001s002` (among others). Matching up to the `mapping.csv`, you can tell this is for `ocean_intercept_regionRegion01`\n",
    ">\n",
    "> To post-process these files to get the jacobian, you just need to subtract 400ppm off them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ef315de-08b7-42ec-a88a-9f24d2792f31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mf_and_flux_for_split(source, lev, split, lat=None, lon=None, pft=None, substr=None, resample='1D', return_flux=True, return_mf=True, quiet=False):\n",
    "    '''\n",
    "    Reads, resamples, and returns mole fraction and flux data for a chosen emission source, \n",
    "    either globally or for a single lat/lon position\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    source : str\n",
    "        either 'ocean', 'gpp', or 'resp'\n",
    "    lev : int\n",
    "        vertical level\n",
    "    split : int\n",
    "        an integer giving the time split number\n",
    "    lat : float, optional\n",
    "        latitude to extract data across time\n",
    "    lon : float, optional\n",
    "        longitude to extract data across time\n",
    "    pft : int, optional\n",
    "        an integer from 1 to 15 giving the plant functional type. Required if source=gpp or resp.\n",
    "    resample : str, optional\n",
    "        a valid time string to pass to xarray.resample. Default is '1D', which causes the returned data\n",
    "        to be resampled to a daily frequency\n",
    "    '''\n",
    "\n",
    "    if(source == 'gpp' or source == 'resp'):\n",
    "        assert pft is not None, 'pft must be supplied if source if gpp or resp'\n",
    "        pfx, sfx = 'sib4_', f'_pft{pft:02d}'\n",
    "        if(source == 'resp'): source = 'resp_tot'\n",
    "    elif(source == 'ocean'): \n",
    "        source = 'ocean_lschulz'\n",
    "        pfx, sfx = '', ''\n",
    "    assert ((lon is None and lat is None) or (lat is not None and lon is not None)), \\\n",
    "            \"lat and lon must both be supplied or not\"\n",
    "\n",
    "    # get mapping file\n",
    "    mapping = pd.read_csv(f'{gc_transport_dir}/mapping.csv')\n",
    "    \n",
    "    # lambda function for formatting file names per input component\n",
    "    fname = lambda component: f'control_{pfx}{source}_{component}{sfx}'\n",
    "    \n",
    "    # make dict of component labels -> component names\n",
    "    if(source == 'ocean_lschulz'):\n",
    "        comp_labels = ['intercept', 'trend', 'sin1', 'sin2', 'cos1', 'cos2', 'residual']\n",
    "        comp_names  = dict(zip(comp_labels, ['intercept', 'trend', 'sin12_1', 'sin12_2', \n",
    "                                             'cos12_1', 'cos12_2', 'residual']))\n",
    "    else:\n",
    "        comp_labels = ['intercept', 'trend', 'sin1', 'sin2', 'sin3', 'cos1', 'cos2', 'cos3', 'residual']\n",
    "        comp_names  = dict(zip(comp_labels, ['intercept', 'trend', 'sin12_1', 'sin12_2', 'sin12_3', \n",
    "                                             'cos12_1', 'cos12_2', 'cos12_3', 'residual']))\n",
    "    \n",
    "    # make dict of component labels -> maps\n",
    "    comp_maps = dict(zip(comp_labels, [mapping[mapping['basis_function'] == fname(comp_names[comp])] \n",
    "                                   for comp in comp_labels]))\n",
    "    comp_runs = dict(zip(comp_labels, [comp_maps[comp]['run'].iat[0] for comp in comp_labels]))\n",
    "    comp_species = dict(zip(comp_labels, [comp_maps[comp]['species'].iat[0] for comp in comp_labels]))\n",
    "\n",
    "    # ----------- get flux field data -----------\n",
    "    # contatenate all files for this split\n",
    "    # coarsen 1-hourly data to daily-mean\n",
    "    if(return_flux):\n",
    "        print(f'reading flux data for level {lev} = {lev_to_p(lev)} hPa = {lev_to_z(lev)} km')\n",
    "        start_time = timeit.default_timer()\n",
    "        comp_flux_data = {}\n",
    "        for i,comp in enumerate(comp_labels):\n",
    "            data_files = sorted(glob.glob(f'{gc_transport_dir}/{comp_runs[comp]}_split{split:02d}'\\\n",
    "                                           '/OutputDir/HEMCO_diagnostics*'))\n",
    "            if(substr is not None):\n",
    "                data_files = np.array(data_files)[[substr in v for v in data_files]]\n",
    "            \n",
    "            N    = len(data_files)\n",
    "            data = [0]*N\n",
    "            for j in range(N):\n",
    "                if(not quiet):\n",
    "                    print(f'reading {source} flux {comp} file {j+1}/{N}...'+''.join([' ']*30), end='\\r')\n",
    "                data_1hr = xr.open_dataset(data_files[j])[f'Emis_{comp_species[comp]}']\n",
    "                data_1hr = data_1hr.isel(lev=lev-1)\n",
    "                if(lat is not None):\n",
    "                    data_1hr = data_1hr.sel(lat=lat, lon=lon, method='nearest')\n",
    "                #print(f'resampling...'+''.join([' ']*50), end='\\r')\n",
    "                data[j] = data_1hr.resample(time=resample).mean()\n",
    "                data[j] = (data[j]* 1e3 * 86400) # convert to g m2/day from kg m2/s\n",
    "            #print(f'concatenating {comp} data ({N} files)...')\n",
    "            data = xr.concat(data, dim='time')\n",
    "            comp_flux_data[comp] = data\n",
    "        \n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(f'took {elapsed:.2f} s')\n",
    "\n",
    "    \n",
    "    # ----------- get mole fraction data -----------\n",
    "    # contatenate all files for this split\n",
    "    # coarsen 3-hourly data to daily-mean\n",
    "    # scale to ppm and remove 400ppm IC\n",
    "    if(return_mf):\n",
    "        print(f'reading mole fraction data for level {lev} = {lev_to_p(lev)} hPa = {lev_to_z(lev)} km')\n",
    "        start_time = timeit.default_timer()\n",
    "        comp_mf_data = {}\n",
    "        for i,comp in enumerate(comp_labels):\n",
    "            data_files = sorted(glob.glob(f'{gc_transport_dir}/{comp_runs[comp]}_split{split:02d}'\\\n",
    "                                           '/OutputDir/GEOSChem.SpeciesConcThreeHourly*'))\n",
    "            if(substr is not None):\n",
    "                data_files = np.array(data_files)[[substr in v for v in data_files]]\n",
    "                \n",
    "            N    = len(data_files)\n",
    "            data = [0]*N\n",
    "            for j in range(N):\n",
    "                if(not quiet):\n",
    "                    print(f'reading {source} mole fraction {comp} file {j+1}/{N}...'+''.join([' ']*30), end='\\r')\n",
    "                data_3hr = xr.open_dataset(data_files[j])[f'SpeciesConcVV_{comp_species[comp]}']\n",
    "                data_3hr = data_3hr.isel(lev=lev-1)\n",
    "                if(lat is not None):\n",
    "                    data_3hr = data_3hr.sel(lat=lat, lon=lon, method='nearest')\n",
    "                #print(f'resampling...'+''.join([' ']*50), end='\\r')\n",
    "                data[j] = data_3hr.resample(time=resample).mean()\n",
    "                data[j] = (data[j]*1e6) - 400 # convert to ppm and subtract 400ppm\n",
    "            #print(f'concatenating {comp} data ({N} files)...')\n",
    "            data = xr.concat(data, dim='time')\n",
    "            comp_mf_data[comp] = data\n",
    "                \n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        print(f'took {elapsed:.2f} s')\n",
    "            \n",
    "    print('done')\n",
    "\n",
    "    # ------ format return dicts as xr Datasets\n",
    "    comp_mf_data   = xr.Dataset(comp_mf_data)\n",
    "    comp_flux_data = xr.Dataset(comp_flux_data)\n",
    "\n",
    "    # ------ done; return\n",
    "    if(return_flux and return_mf): return comp_mf_data, comp_flux_data\n",
    "    elif(return_flux):             return comp_flux_data\n",
    "    elif(rturn_mf):                return comp_mf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bfd3ad-5cce-41da-b3b3-55c8e15951b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f0642e-b289-4081-9c65-0b6a6a5e3556",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad564c4-375d-441f-b653-5c844259e979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading flux data for level 1 = 1005.65 hPa = 0.058 km\n",
      "took 300.44 s_lschulz flux residual file 4/4...                               \n",
      "reading mole fraction data for level 1 = 1005.65 hPa = 0.058 km\n",
      "took 297.48 s_lschulz mole fraction residual file 120/120...                               \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# get ocean fluxes and mf response for Jan to April, 2021, PFT 1\n",
    "ocean_mf_surf, ocean_flux_surf = get_mf_and_flux_for_split(source='ocean', lev=1, split=20)#, substr='202101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ec7098b0-e086-464b-9de3-c4548bfbf7c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading flux data for level 23 = 510.475 hPa = 5.413 km\n",
      "took 13.11 sn_lschulz flux residual file 4/4...                               \n",
      "reading mole fraction data for level 23 = 510.475 hPa = 5.413 km\n",
      "took 114.99 s_lschulz mole fraction residual file 120/120...                               \n",
      "done\n"
     ]
    }
   ],
   "source": [
    "ocean_mf_5km, ocean_flux_5km = get_mf_and_flux_for_split(source='ocean', lev=23, split=20)#, substr='202101')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "65fbc3a1-e2c8-4679-ae0b-13b7acba7af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "aalat, aalon = 42, 277\n",
    "ilev=10\n",
    "\n",
    "timef = ocean_flux_surf['trend'].time\n",
    "time = ocean_mf_surf['trend'].time\n",
    "days = np.arange(len(time))\n",
    "days = days[:, None, None, None]\n",
    "lat  = ocean_mf_surf['trend'].lat\n",
    "lon  = ocean_mf_surf['trend'].lon\n",
    "lev  = ocean_mf_surf['trend'].lev\n",
    "\n",
    "if(0):\n",
    "    # get column-integrated\n",
    "    print('copying column-integrated')\n",
    "    x_data = copy.deepcopy(comp_data)\n",
    "    print('copying column-integrated scaled')\n",
    "    x_data_scaled = copy.deepcopy(data_scaled)\n",
    "    print('computing column-integrated')\n",
    "    for comp in comp_labels:\n",
    "        x_data[comp] = comp_data[comp].sum('lev')\n",
    "        x_data_scaled[comp] = data_scaled[comp].sum('lev')\n",
    "    \n",
    "    #get zonal mean\n",
    "    print('copying zonal means')\n",
    "    zm_data = copy.deepcopy(comp_data)\n",
    "    print('copying zonal means scaled')\n",
    "    zm_data_scaled = copy.deepcopy(data_scaled)\n",
    "    print('computing zonal means')\n",
    "    for comp in comp_labels:\n",
    "        zm_data[comp] = comp_data[comp].mean('lon')\n",
    "        zm_data_scaled[comp] = data_scaled[comp].mean('lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d191ab80-8f37-48e1-8866-45ed58118ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "118\r"
     ]
    }
   ],
   "source": [
    "#matplotlib.use('Agg')\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "component='trend'\n",
    "figdir = '/home/jhollo/repos/climate_analysis/OCO/figs'\n",
    "\n",
    "if(component == 'intercept'):\n",
    "    flux_levels = np.linspace(-1, 1, 11)\n",
    "    mf_levels   = [-12, -10, -8, -6, -4, -2, 0, 1, 2, 3]\n",
    "    mf_levels = np.arange(-10.5, -7, 0.5)\n",
    "if(component == 'trend'):\n",
    "    flux_levels = np.linspace(-0.1, 0.1, 11)\n",
    "    mf_levels   = [-12, -10, -8, -6, -4, -2, 0, 1, 2, 3]\n",
    "    mf_levels = np.arange(-0.2, 0.2, 0.04)\n",
    "\n",
    "for i in range(120):\n",
    "    if(i%2 != 0): continue\n",
    "    print(i, end='\\r')\n",
    "    fig = plt.figure(figsize=(15,5))\n",
    "    ax_flux = fig.add_subplot(121, projection=ccrs.Mollweide())\n",
    "    ax_mf   = fig.add_subplot(122, projection=ccrs.Mollweide())\n",
    "    \n",
    "    ccf = ax_flux.contourf(lon, lat, (ocean_flux_surf[component]).isel(time=i), transform=ccrs.PlateCarree(), \n",
    "                     levels=flux_levels, cmap='bwr', extend='both')\n",
    "    cb  = plt.colorbar(ccf, location='bottom', pad=0.02)\n",
    "    cb.set_label(r'g m$^2$ day$^{-1}$', fontsize=14)\n",
    "    \n",
    "    ccmf = ax_mf.contourf(lon, lat, (ocean_mf_surf[component]).isel(time=i), transform=ccrs.PlateCarree(), \n",
    "                   levels=mf_levels, cmap='Spectral_r', extend='both')#norm=TwoSlopeNorm(vcenter=0))\n",
    "    cb   = plt.colorbar(ccmf, location='bottom', pad=0.02)\n",
    "    cb.set_label(r'ppm', fontsize=14)\n",
    "    \n",
    "    for ax in [ax_flux, ax_mf]:\n",
    "        ax.coastlines()\n",
    "        ax.set_global()\n",
    "    ax_flux.set_title(f'flux {component}')\n",
    "    ax_mf.set_title(f'mole fraction {component}')\n",
    "    fig.suptitle(pd.Timestamp(time.values[i]).strftime('%Y-%m-%d'), y=0.9)\n",
    "    plt.savefig(f'{figdir}/{component}_t{i:03d}.png', dpi=200)\n",
    "    plt.close(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "miniconda base",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
